{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\"> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> <h1>Natural Language Processing</h1>\n",
    "<h1>Sequence Modeling</h1>\n",
    "        <p>Bruno Gon√ßalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "            @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moNmVfuvnImW"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gzip\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "RANDOM_SEED = 123\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchview\n",
    "from torchview import draw_graph\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by print out the versions of the libraries we're using for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.18\n",
      "IPython version      : 8.38.0\n",
      "\n",
      "Compiler    : Clang 20.1.4 \n",
      "OS          : Darwin\n",
      "Release     : 25.2.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 008980780627058467840b5f9dd419dfbb750104\n",
      "\n",
      "matplotlib: 3.10.8\n",
      "numpy     : 1.26.4\n",
      "watermark : 2.4.3\n",
      "torchview : 0.2.7\n",
      "torch     : 1.11.0\n",
      "pandas    : 2.3.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load default figure style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('d4sci.mplstyle')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper classes for vocabulary and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<unk>\"}\n",
    "        self.word_counts = Counter()\n",
    "        \n",
    "    def build_vocab(self, texts, max_size=None):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        for text in texts:\n",
    "            tokens = text.lower().split()\n",
    "            self.word_counts.update(tokens)\n",
    "        \n",
    "        # Get most common words\n",
    "        if max_size:\n",
    "            most_common = self.word_counts.most_common(max_size - 2)  # -2 for <pad> and <unk>\n",
    "        else:\n",
    "            most_common = self.word_counts.most_common()\n",
    "        \n",
    "        # Add to vocabulary\n",
    "        for idx, (word, count) in enumerate(most_common, start=2):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to indices\"\"\"\n",
    "        tokens = text.lower().split()\n",
    "        return [self.word2idx.get(token, 1) for token in tokens]  # 1 is <unk>\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.vocab:\n",
    "            text_indices = self.vocab.encode(text)\n",
    "        else:\n",
    "            text_indices = text\n",
    "            \n",
    "        return torch.tensor(text_indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"Collate function for DataLoader to handle variable length sequences\"\"\"\n",
    "    texts, labels = zip(*batch)\n",
    "    \n",
    "    # Pad sequences\n",
    "    texts_padded = pad_sequence(texts, batch_first=False, padding_value=0)  # batch_first=False for RNN\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return texts_padded, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 50000\n",
      "Sample data:\n",
      "                                              review sentiment  label\n",
      "0  One of the other reviewers has mentioned that ...  positive      1\n",
      "1  A wonderful little production. <br /><br />The...  positive      1\n",
      "2  I thought this was a wonderful way to spend ti...  positive      1\n",
      "3  Basically there's a family where a little boy ...  negative      0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('data/IMDB Dataset.csv')\n",
    "\n",
    "# Convert labels to numeric (0 = negative, 1 = positive)\n",
    "label_map = {'negative': 0, 'positive': 1}\n",
    "df['label'] = df['sentiment'].map(label_map)\n",
    "\n",
    "print(f'Dataset size: {len(df)}')\n",
    "print(f'Sample data:\\n{df.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation, and test partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WZ_4jiHVnMxN",
    "outputId": "dfa51c04-4845-44c3-f50b-d36d41f132b8"
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    df['review'].values, \n",
    "    df['label'].values, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Second split: 85% train, 15% validation (of the train+val set)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X_train_val, \n",
    "    y_train_val, \n",
    "    test_size=0.15, \n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_train_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: 34000\n",
      "Shape of test data: 10000\n",
      "Shape of validation data: 6000\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training data:', len(X_train))\n",
    "print('Shape of test data:', len(X_test))\n",
    "print('Shape of validation data:', len(X_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-TBwKWPslPa"
   },
   "source": [
    "Build the vocabulary based on the top \"vocabulary_size\" words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "e8uNrjdtn4A8",
    "outputId": "6cf499d7-7722-4da0-8576-ee0f218cc6e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20000\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "TEXT_vocab = Vocabulary()\n",
    "TEXT_vocab.build_vocab(X_train, max_size=vocabulary_size)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT_vocab)}')\n",
    "print(f'Number of classes: 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIQ_zfKLwjKm"
   },
   "source": [
    "## Define Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Check for available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7JiHR1stHNF"
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = TextDataset(X_train, y_train, TEXT_vocab)\n",
    "valid_dataset = TextDataset(X_validation, y_validation, TEXT_vocab)\n",
    "test_dataset = TextDataset(X_test, y_test, TEXT_vocab)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to define our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN model\n",
    "\n",
    "As the structure of the model is the same for RNN, GRU, and LSTMs, we implement just one of them for demonstration purposes. We can easily plugin another type of layer as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQIUm5EjxFNa"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size)        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, text):        \n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        hidden.squeeze_(0)        \n",
    "        output = self.fc(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 256\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ik3NF3faxFmZ"
   },
   "outputs": [],
   "source": [
    "model = RNN(len(TEXT_vocab),  # vocabulary size\n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            num_classes \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(20000, 128)\n",
      "  (rnn): LSTM(128, 256)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ik3NF3faxFmZ"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lv9Ny9di6VcI"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T5t1Afn4xO11"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for features, targets in data_loader:\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "            \n",
    "    return correct_pred.float()/num_examples * 100, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this [notebook](https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1836
    },
    "colab_type": "code",
    "id": "EABZM8Vo0ilB",
    "outputId": "5d45e293-9909-4588-e793-8dfaf72e5c67"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cc7e037fc74923b6d0af5980279a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1:   0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/266 | Loss: 0.6930\n",
      "Epoch: 001/015 | Batch 050/266 | Loss: 0.6895\n",
      "Epoch: 001/015 | Batch 100/266 | Loss: 0.6935\n",
      "Epoch: 001/015 | Batch 150/266 | Loss: 0.6884\n",
      "Epoch: 001/015 | Batch 200/266 | Loss: 0.6938\n",
      "Epoch: 001/015 | Batch 250/266 | Loss: 0.6923\n",
      "training accuracy: 50.11%\n",
      "valid accuracy: 50.03%\n",
      "Time elapsed: 9.83 min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad99d5804714ad08ce400f9909a504d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2:   0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002/015 | Batch 000/266 | Loss: 0.6941\n",
      "Epoch: 002/015 | Batch 050/266 | Loss: 0.6921\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'epochs': n_epochs\n",
    "    }\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for batch_idx, (text, labels) in tqdm(enumerate(train_loader), \n",
    "                                      total=len(train_loader), \n",
    "                                      desc='Epoch: %u' % (epoch+1),\n",
    "                                      leave=True):\n",
    "        \n",
    "        text = text.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(text)\n",
    "        \n",
    "        # Back propagation\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{n_epochs:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, device)[0]:.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, device)[0]:.2f}%')\n",
    "    \n",
    "    history['train_loss'].append(loss.to('cpu').detach().numpy())\n",
    "    history['train_acc'].append(compute_accuracy(model, train_loader, device)[0].to('cpu').detach().numpy())\n",
    "    \n",
    "    val_acc, val_loss = compute_accuracy(model, valid_loader, device)\n",
    "    \n",
    "    history['val_acc'].append(val_acc.to('cpu').detach().numpy())\n",
    "    history['val_loss'].append(val_loss.to('cpu').detach().numpy())\n",
    "\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, device)[0]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):    \n",
    "    fig, ax_lst = plt.subplots(1, 2, sharex=True, sharey=False)\n",
    "    \n",
    "    epochs = range(history['epochs'])\n",
    "    \n",
    "    ax_lst[0].plot(epochs, history['train_loss'], label='Training')\n",
    "    ax_lst[0].plot(epochs, history['val_loss'], label='Testing')\n",
    "    ax_lst[0].set_ylabel('Loss')\n",
    "    ax_lst[0].set_xlabel('Epoch')\n",
    "    ax_lst[0].set_xticks(epochs)\n",
    "    \n",
    "    ax_lst[1].plot(epochs, history['train_acc'], label='Training')\n",
    "    ax_lst[1].plot(epochs, history['val_acc'], label='Testing')\n",
    "    ax_lst[1].set_ylabel('Accuracy')\n",
    "    ax_lst[1].set_xlabel('Epoch')\n",
    "    ax_lst[1].set_xticks(epochs)\n",
    "\n",
    "    ax_lst[1].legend()\n",
    "    \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "     <img src=\"data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_lstm_packed_imdb.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
